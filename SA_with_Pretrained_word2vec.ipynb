{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import những thư viện cần thiết\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "from pyvi import ViTokenizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Load mô hình trained word2vec để embedding\n",
    "word2vec_model_path = (\"data/vi.vec\")\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(word2vec_model_path)\n",
    "vocab = w2v.wv.vocab\n",
    "model = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('./data/baomoi.vn.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data file\n",
    "\n",
    "training_data = load_files(r\"./data2\", encoding=\"utf-8\")\n",
    "X, Y = training_data.data, training_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file chứa các từ stop_words\n",
    "\n",
    "stop_ws = []\n",
    "with open(\"vnstopword.txt\", encoding=\"utf-8\") as f :\n",
    "    text = f.read()\n",
    "    for word in text.split('\\n') :  #Tách ra mỗi dòng là 1 từ stopword riêng lẻ\n",
    "        stop_ws.append(word)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#=====================================================================================\n",
    "# Tiền xử lý cho tập dữ liệu training\n",
    "documents = []\n",
    "sum_word = 0\n",
    "for sen in range(0, len(X)):\n",
    "    \n",
    "    # remove stop word\n",
    "    document = []\n",
    "    for word in str(X[sen]).split() :\n",
    "        sum_word = sum_word + 1         # Đếm tổng số từ trong tập dữ liệu\n",
    "        if (word not in stop_ws) :\n",
    "            if (\"_\" in word) or (word.isalpha() == True):\n",
    "                document.append(word)\n",
    "    #print(document)\n",
    "    document = ' '.join(document)\n",
    "\n",
    "    replace_list = {\n",
    "        'òa': 'oà', 'óa': 'oá', 'ỏa': 'oả', 'õa': 'oã', 'ọa': 'oạ', 'òe': 'oè', 'óe': 'oé','ỏe': 'oẻ',\n",
    "        'õe': 'oẽ', 'ọe': 'oẹ', 'ùy': 'uỳ', 'úy': 'uý', 'ủy': 'uỷ', 'ũy': 'uỹ','ụy': 'uỵ', 'uả': 'ủa',\n",
    "        'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\n",
    "        'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\n",
    "        'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\n",
    "        ' okey ': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\n",
    "        ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ',\n",
    "        ' kg ': u' không ','not': u' không ', u' kg ': u' không ', ' k ': u' không ',' kh ':u' không ',' kô ':u' không ',' hok ':u' không ',' kp ': u' không phải ',u' kô ': u' không ', ' ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',' hong ':u' không',\n",
    "        'vs': u' với ', 'wa': ' quá ', 'wá': u' quá', 'j': u' gì ', '“': ' ',' đx ': u' được ', 'dk': u' được ', 'dc': u' được ', 'đk': u' được ',\n",
    "        'đc': u' được ',' thick ': u' thích ', 'store': u' cửa hàng ',\n",
    "        'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ',\n",
    "        ' sấu ': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\n",
    "        'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\n",
    "        'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\n",
    "        'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\n",
    "        'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\n",
    "        'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',u'quả ng ':u' quảng  ',\n",
    "        ' dep ': u' đẹp ',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u'qủa': u' quả ','iu': u' yêu ','pv':u' phục vụ','nv':u' nhân viên','nc':u' nước',\n",
    "        u' hó ':u' khó',u' hắc ':u' khắc ','_': ' ', u' ng ':u' người '}\n",
    "\n",
    "    for k, v in replace_list.items():\n",
    "        document = document.replace(k, v)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Loại bỏ Hashtag\n",
    "    document = re.sub(r'#([^\\s]+)', '', document)\n",
    "    # Loại bỏ các URL\n",
    "    document = re.sub(r'http([^\\s]+)', '', document)\n",
    "    # Loại bỏ number có trong comment\n",
    "    document = re.sub(r'\\d+', '', document)\n",
    "    #Remove duplicated characters: vd: đẹppppppp\n",
    "    document = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), document, flags=re.IGNORECASE)\n",
    "    # Loại bỏ các dấu câu, các ký tự đặc biệt (Emoji, icon cũng bị remove)\n",
    "    document = re.sub(r'[^\\w\\s]','', document)\n",
    "    # Chuyển chữ in hoa thành chữ thường\n",
    "    document = document.lower()\n",
    "    # Loại bỏ các ký tự dư thừa đơn lẻ (Nhiều ký tự đơn lẻ liền nhau kg xử lý được)\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Loại bỏ các Emoji, icon\n",
    "    document = re.sub(r'[^\\w\\s,]', '', document)\n",
    "    # Loại bỏ nhiều khoảng trắng dư thừa\n",
    "     #document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Tokenization\n",
    "    document = ViTokenizer.tokenize(document)\n",
    "    \n",
    "   \n",
    "    \n",
    "    documents.append(document.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.most_similar(\"phục_vụ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm vector tất cả các word bằng word2vec model\n",
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through each reviews\n",
    "    for review in reviews:\n",
    "         # Print a status message every 1000th review\n",
    "        if counter%100. == 0:\n",
    "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "            reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "     \n",
    "        counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 10000\n",
      "Review 100 of 10000\n",
      "Review 200 of 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 300 of 10000\n",
      "Review 400 of 10000\n",
      "Review 500 of 10000\n",
      "Review 600 of 10000\n",
      "Review 700 of 10000\n",
      "Review 800 of 10000\n",
      "Review 900 of 10000\n",
      "Review 1000 of 10000\n",
      "Review 1100 of 10000\n",
      "Review 1200 of 10000\n",
      "Review 1300 of 10000\n",
      "Review 1400 of 10000\n",
      "Review 1500 of 10000\n",
      "Review 1600 of 10000\n",
      "Review 1700 of 10000\n",
      "Review 1800 of 10000\n",
      "Review 1900 of 10000\n",
      "Review 2000 of 10000\n",
      "Review 2100 of 10000\n",
      "Review 2200 of 10000\n",
      "Review 2300 of 10000\n",
      "Review 2400 of 10000\n",
      "Review 2500 of 10000\n",
      "Review 2600 of 10000\n",
      "Review 2700 of 10000\n",
      "Review 2800 of 10000\n",
      "Review 2900 of 10000\n",
      "Review 3000 of 10000\n",
      "Review 3100 of 10000\n",
      "Review 3200 of 10000\n",
      "Review 3300 of 10000\n",
      "Review 3400 of 10000\n",
      "Review 3500 of 10000\n",
      "Review 3600 of 10000\n",
      "Review 3700 of 10000\n",
      "Review 3800 of 10000\n",
      "Review 3900 of 10000\n",
      "Review 4000 of 10000\n",
      "Review 4100 of 10000\n",
      "Review 4200 of 10000\n",
      "Review 4300 of 10000\n",
      "Review 4400 of 10000\n",
      "Review 4500 of 10000\n",
      "Review 4600 of 10000\n",
      "Review 4700 of 10000\n",
      "Review 4800 of 10000\n",
      "Review 4900 of 10000\n",
      "Review 5000 of 10000\n",
      "Review 5100 of 10000\n",
      "Review 5200 of 10000\n",
      "Review 5300 of 10000\n",
      "Review 5400 of 10000\n",
      "Review 5500 of 10000\n",
      "Review 5600 of 10000\n",
      "Review 5700 of 10000\n",
      "Review 5800 of 10000\n",
      "Review 5900 of 10000\n",
      "Review 6000 of 10000\n",
      "Review 6100 of 10000\n",
      "Review 6200 of 10000\n",
      "Review 6300 of 10000\n",
      "Review 6400 of 10000\n",
      "Review 6500 of 10000\n",
      "Review 6600 of 10000\n",
      "Review 6700 of 10000\n",
      "Review 6800 of 10000\n",
      "Review 6900 of 10000\n",
      "Review 7000 of 10000\n",
      "Review 7100 of 10000\n",
      "Review 7200 of 10000\n",
      "Review 7300 of 10000\n",
      "Review 7400 of 10000\n",
      "Review 7500 of 10000\n",
      "Review 7600 of 10000\n",
      "Review 7700 of 10000\n",
      "Review 7800 of 10000\n",
      "Review 7900 of 10000\n",
      "Review 8000 of 10000\n",
      "Review 8100 of 10000\n",
      "Review 8200 of 10000\n",
      "Review 8300 of 10000\n",
      "Review 8400 of 10000\n",
      "Review 8500 of 10000\n",
      "Review 8600 of 10000\n",
      "Review 8700 of 10000\n",
      "Review 8800 of 10000\n",
      "Review 8900 of 10000\n",
      "Review 9000 of 10000\n",
      "Review 9100 of 10000\n",
      "Review 9200 of 10000\n",
      "Review 9300 of 10000\n",
      "Review 9400 of 10000\n",
      "Review 9500 of 10000\n",
      "Review 9600 of 10000\n",
      "Review 9700 of 10000\n",
      "Review 9800 of 10000\n",
      "Review 9900 of 10000\n"
     ]
    }
   ],
   "source": [
    "# Wordembeding toàn bộ text_data, dựa trên model word2vec đã tạo ở trên\n",
    "trainDataVecs = getAvgFeatureVecs(documents,model,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(trainDataVecs, Y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  0.029984712600708008  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng NaiveBayes\n",
    "\n",
    "import time\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_NB = time.time()\n",
    "\n",
    "\n",
    "classifier_NB = GaussianNB()\n",
    "classifier_NB.fit(X_Train, Y_Train)\n",
    "\n",
    "end_NB = time.time()\n",
    "time_NB = end_NB-start_NB\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_NB), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  21.681555032730103  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng Random Forest\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_RF = time.time()\n",
    "\n",
    "classifier_RF = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier_RF.fit(X_Train, Y_Train) \n",
    "\n",
    "end_RF = time.time()\n",
    "time_RF = end_RF-start_RF\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_RF), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  24.685691356658936  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng SVM\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_SVM = time.time()\n",
    "\n",
    "classifier_SVM = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_SVM.fit(X_Train, Y_Train)\n",
    "\n",
    "\n",
    "end_SVM = time.time()\n",
    "time_SVM = end_SVM-start_SVM\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_SVM), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện Logistic Regression:  17.465168952941895  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "\n",
    "classifier_LR = LogisticRegression(fit_intercept=True, n_jobs=4)\n",
    "classifier_LR.fit(X_Train, Y_Train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Thời gian huấn luyện Logistic Regression: \", str(end-start), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện KNN:  7.823147535324097  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier_KNN.fit(X_Train, Y_Train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Thời gian huấn luyện KNN: \", str(end-start), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Sentiment\n",
    "\n",
    "y_pred_NB = classifier_NB.predict(X_Test)\n",
    "\n",
    "y_pred_RF = classifier_RF.predict(X_Test)\n",
    "\n",
    "y_pred_SVM = classifier_SVM.predict(X_Test)\n",
    "\n",
    "#y_Pred_LR = classifier_LR.predict(X_Test)\n",
    "\n",
    "#y_Pred_KNN = classifier_KNN.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.5076666666666667\n",
      "Accuracy of Random Forest: 0.49333333333333335\n",
      "Accuracy of SVM: 0.493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#print('Accuracy LogisticRegression: ', accuracy_score(Y_Test, y_Pred_LR))\n",
    "print('Accuracy of Naive Bayes:',accuracy_score(Y_Test, y_pred_NB))\n",
    "#print('Accuracy KNN: ', accuracy_score(Y_Test, y_Pred_KNN))\n",
    "print('Accuracy of Random Forest:',accuracy_score(Y_Test, y_pred_RF))\n",
    "print('Accuracy of SVM:',accuracy_score(Y_Test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LogisticRegression:  0.49855555555555553\n",
      "Accuracy of Naive Bayes: 0.49677777777777776\n",
      "Accuracy KNN:  0.5053333333333333\n",
      "Accuracy of Random Forest: 0.4978888888888889\n",
      "Accuracy of SVM: 0.4981111111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#print('Accuracy LogisticRegression: ', accuracy_score(Y_Test, y_Pred_LR))\n",
    "print('Accuracy of Naive Bayes:',accuracy_score(Y_Test, y_pred_NB))\n",
    "#print('Accuracy KNN: ', accuracy_score(Y_Test, y_Pred_KNN))\n",
    "print('Accuracy of Random Forest:',accuracy_score(Y_Test, y_pred_RF))\n",
    "print('Accuracy of SVM:',accuracy_score(Y_Test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Training time of NB:',time_NB)\n",
    "#print('Training time of RF:',time_RF)\n",
    "#print('Training time of SVM:',time_SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
