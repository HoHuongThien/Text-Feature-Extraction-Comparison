{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/alaakh42/Sentiment_Model_Template/blob/master/Sentiment_using_pretrained_models.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install lightgbm\n",
    "# pip install BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import gensim\n",
    "from tqdm import tqdm\n",
    "#from BeautifulSoup import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import gc\n",
    "import scipy\n",
    "\n",
    "# Models For Experimentation\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, LinearSVR\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "#import lightgbm as lgb \n",
    "#import xgboost as xgb\n",
    "\n",
    "# For Model Assessment\n",
    "from sklearn.metrics import classification_report, accuracy_score, auc\n",
    "\n",
    "# Data Split\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from pyvi import ViTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data file\n",
    "\n",
    "training_data = load_files(r\"./data2\", encoding=\"utf-8\")\n",
    "X, y = training_data.data, training_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file chứa các từ stop_words\n",
    "\n",
    "stop_ws = []\n",
    "with open(\"vnstopword.txt\", encoding=\"utf-8\") as f :\n",
    "    text = f.read()\n",
    "    for word in text.split('\\n') :  #Tách ra mỗi dòng là 1 từ stopword riêng lẻ\n",
    "        stop_ws.append(word)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiền xử lý cho tập dữ liệu training\n",
    "documents = []\n",
    "sum_word = 0\n",
    "for sen in range(0, len(X)):\n",
    "    \n",
    "    # remove stop word\n",
    "    document = []\n",
    "    for word in str(X[sen]).split() :\n",
    "        sum_word = sum_word + 1         # Đếm tổng số từ trong tập dữ liệu\n",
    "        if (word not in stop_ws) :\n",
    "            if (\"_\" in word) or (word.isalpha() == True):\n",
    "                document.append(word)\n",
    "    #print(document)\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    replace_list = {\n",
    "        'òa': 'oà', 'óa': 'oá', 'ỏa': 'oả', 'õa': 'oã', 'ọa': 'oạ', 'òe': 'oè', 'óe': 'oé','ỏe': 'oẻ',\n",
    "        'õe': 'oẽ', 'ọe': 'oẹ', 'ùy': 'uỳ', 'úy': 'uý', 'ủy': 'uỷ', 'ũy': 'uỹ','ụy': 'uỵ', 'uả': 'ủa',\n",
    "        'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\n",
    "        'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\n",
    "        'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\n",
    "        ' okey ': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\n",
    "        ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ',\n",
    "        ' kg ': u' không ','not': u' không ', u' kg ': u' không ', ' k ': u' không ',' kh ':u' không ',' kô ':u' không ',' hok ':u' không ',' kp ': u' không phải ',u' kô ': u' không ', ' ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',' hong ':u' không',\n",
    "        'vs': u' với ', 'wa': ' quá ', 'wá': u' quá', 'j': u' gì ', '“': ' ',' đx ': u' được ', 'dk': u' được ', 'dc': u' được ', 'đk': u' được ',\n",
    "        'đc': u' được ',' thick ': u' thích ', 'store': u' cửa hàng ',\n",
    "        'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ',\n",
    "        ' sấu ': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\n",
    "        'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\n",
    "        'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\n",
    "        'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\n",
    "        'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\n",
    "        'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',u'quả ng ':u' quảng  ',\n",
    "        ' dep ': u' đẹp ',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u'qủa': u' quả ','iu': u' yêu ','pv':u' phục vụ','nv':u' nhân viên','nc':u' nước',\n",
    "        u' hó ':u' khó',u' hắc ':u' khắc ','_': ' ', u' ng ':u' người '}\n",
    "\n",
    "    for k, v in replace_list.items():\n",
    "        document = document.replace(k, v)\n",
    "\n",
    "    # Loại bỏ Hashtag\n",
    "    #document = re.sub(r'#([^\\s]+)', '', str(X[sen]))\n",
    "    document = re.sub(r'#([^\\s]+)', '', document)\n",
    "    # Loại bỏ các URL\n",
    "    document = re.sub(r'http([^\\s]+)', '', document)\n",
    "    # Loại bỏ number có trong comment\n",
    "    document = re.sub(r'\\d+', '', document)\n",
    "    # Loại bỏ các dấu câu, các ký tự đặc biệt (Emoji, icon cũng bị remove)\n",
    "    document = re.sub(r'[^\\w\\s]','', document)\n",
    "    # Chuyển chữ in hoa thành chữ thường\n",
    "    document = document.lower()\n",
    "    # Loại bỏ các ký tự dư thừa đơn lẻ (Nhiều ký tự đơn lẻ liền nhau kg xử lý được)\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Loại bỏ các Emoji, icon\n",
    "    document = re.sub(r'[^\\w\\s,]', '', document)\n",
    "    # Loại bỏ nhiều khoảng trắng dư thừa\n",
    "     #document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Tokenization\n",
    "    document = ViTokenizer.tokenize(document)\n",
    "    \n",
    "    document = ''.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(u'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s):\n",
    "    return re_tok.sub(r' \\1 ', s).split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start by Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1193514it [02:53, 6880.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "embeddings_index_glove = {}\n",
    "f = open('glove.twitter.27B.200d.txt','r', encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index_glove[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a normalized vector for the whole sentence\n",
    "def sent2vec(s, embeddings_index):\n",
    "    words = str(s).lower().encode().decode('utf-8')\n",
    "    words = tokenize(words)\n",
    "    words = [w for w in words if not w in stop_ws]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(200)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 7000/7000 [00:07<00:00, 924.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3000/3000 [00:04<00:00, 614.51it/s]\n"
     ]
    }
   ],
   "source": [
    "#create sentence vectors using the above function for training and validation set\n",
    "\n",
    "xtrain_glove = [sent2vec(x, embeddings_index_glove) for x in tqdm(X_train)]\n",
    "xtest_glove = [sent2vec(x, embeddings_index_glove) for x in tqdm(X_test)]\n",
    "\n",
    "xtrain_glove = np.array(xtrain_glove)\n",
    "xtest_glove = np.array(xtest_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating the features its time to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  0.028980731964111328  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng NaiveBayes\n",
    "\n",
    "import time\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_NB = time.time()\n",
    "\n",
    "\n",
    "#classifier_NB = BernoulliNB(binarize=None)\n",
    "classifier_NB = GaussianNB()\n",
    "classifier_NB.fit(xtrain_glove, y_train)\n",
    "\n",
    "end_NB = time.time()\n",
    "time_NB = end_NB-start_NB\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_NB), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  425.9665598869324  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng Random Forest\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_RF = time.time()\n",
    "\n",
    "classifier_RF = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier_RF.fit(xtrain_glove, y_train) \n",
    "\n",
    "end_RF = time.time()\n",
    "time_RF = end_RF-start_RF\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_RF), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  166.216814994812  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng SVM\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_SVM = time.time()\n",
    "\n",
    "classifier_SVM = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_SVM.fit(xtrain_glove, y_train)\n",
    "\n",
    "\n",
    "end_SVM = time.time()\n",
    "time_SVM = end_SVM-start_SVM\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_SVM), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện Logistic Regression:  0.6775789260864258  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "\n",
    "classifier_LR = LogisticRegression(fit_intercept=True, n_jobs=1)\n",
    "classifier_LR.fit(xtrain_glove, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Thời gian huấn luyện Logistic Regression: \", str(end-start), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện KNN:  2.043734073638916  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier_KNN.fit(xtrain_glove, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Thời gian huấn luyện KNN: \", str(end-start), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Sentiment\n",
    "\n",
    "y_pred_NB = classifier_NB.predict(xtest_glove)\n",
    "\n",
    "#y_pred_RF = classifier_RF.predict(xtest_glove)\n",
    "\n",
    "#y_pred_SVM = classifier_SVM.predict(xtest_glove)\n",
    "\n",
    "#y_Pred_LR = classifier_LR.predict(xtest_glove)\n",
    "\n",
    "#y_Pred_KNN = classifier_KNN.predict(xtest_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes: 0.593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#print('Accuracy LogisticRegression: ', accuracy_score(y_test, y_Pred_LR))\n",
    "print('Accuracy of Naive Bayes:',accuracy_score(y_test, y_pred_NB))\n",
    "#print('Accuracy KNN: ', accuracy_score(y_test, y_Pred_KNN))\n",
    "#print('Accuracy of Random Forest:',accuracy_score(y_test, y_pred_RF))\n",
    "#print('Accuracy of SVM:',accuracy_score(y_test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
