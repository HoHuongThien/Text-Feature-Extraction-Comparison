{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# gensim modules\n",
    "from gensim import utils\n",
    "from smart_open import open\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "import pickle\n",
    "from pyvi import ViTokenizer\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def text_preprocessing(document):\n",
    "   \n",
    "    replace_list = {\n",
    "        'òa': 'oà', 'óa': 'oá', 'ỏa': 'oả', 'õa': 'oã', 'ọa': 'oạ', 'òe': 'oè', 'óe': 'oé','ỏe': 'oẻ',\n",
    "        'õe': 'oẽ', 'ọe': 'oẹ', 'ùy': 'uỳ', 'úy': 'uý', 'ủy': 'uỷ', 'ũy': 'uỹ','ụy': 'uỵ', 'uả': 'ủa',\n",
    "        'ả': 'ả', 'ố': 'ố', 'u´': 'ố','ỗ': 'ỗ', 'ồ': 'ồ', 'ổ': 'ổ', 'ấ': 'ấ', 'ẫ': 'ẫ', 'ẩ': 'ẩ',\n",
    "        'ầ': 'ầ', 'ỏ': 'ỏ', 'ề': 'ề','ễ': 'ễ', 'ắ': 'ắ', 'ủ': 'ủ', 'ế': 'ế', 'ở': 'ở', 'ỉ': 'ỉ',\n",
    "        'ẻ': 'ẻ', 'àk': u' à ','aˋ': 'à', 'iˋ': 'ì', 'ă´': 'ắ','ử': 'ử', 'e˜': 'ẽ', 'y˜': 'ỹ', 'a´': 'á',\n",
    "        ' okey ': ' ok ', 'ôkê': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okê':' ok ',\n",
    "        ' tks ': u' cám ơn ', 'thks': u' cám ơn ', 'thanks': u' cám ơn ', 'ths': u' cám ơn ', 'thank': u' cám ơn ',\n",
    "        ' kg ': u' không ','not': u' không ', u' kg ': u' không ', ' k ': u' không ',' kh ':u' không ',' kô ':u' không ',' hok ':u' không ',' kp ': u' không phải ',u' kô ': u' không ', ' ko ': u' không ', u' ko ': u' không ', u' k ': u' không ', 'khong': u' không ', u' hok ': u' không ',' hong ':u' không',\n",
    "        'vs': u' với ', 'wa': ' quá ', 'wá': u' quá', 'j': u' gì ', '“': ' ',' đx ': u' được ', 'dk': u' được ', 'dc': u' được ', 'đk': u' được ',\n",
    "        'đc': u' được ',' thick ': u' thích ', 'store': u' cửa hàng ',\n",
    "        'shop': u' cửa hàng ', 'sp': u' sản phẩm ', 'gud': u' tốt ','god': u' tốt ','wel done':' tốt ', 'good': u' tốt ', 'gút': u' tốt ',\n",
    "        ' sấu ': u' xấu ','gut': u' tốt ', u' tot ': u' tốt ', u' nice ': u' tốt ', 'perfect': 'rất tốt', 'bt': u' bình thường ',\n",
    "        'time': u' thời gian ', 'qá': u' quá ', u' ship ': u' giao hàng ', u' m ': u' mình ', u' mik ': u' mình ',\n",
    "        'date': u' hạn sử dụng ', 'hsd': u' hạn sử dụng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hàng ',u' síp ': u' giao hàng ',\n",
    "        'beautiful': u' đẹp tuyệt vời ', u' tl ': u' trả lời ', u' r ': u' rồi ', u' shopE ': u' cửa hàng ',u' order ': u' đặt hàng ',\n",
    "        'chất lg': u' chất lượng ',u' sd ': u' sử dụng ',u' dt ': u' điện thoại ',u' nt ': u' nhắn tin ',u' tl ': u' trả lời ',u' sài ': u' xài ',u'bjo':u' bao giờ ',\n",
    "        'thik': u' thích ',u' sop ': u' cửa hàng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' rất ',u'quả ng ':u' quảng  ',\n",
    "        ' dep ': u' đẹp ',u' xau ': u' xấu ','delicious': u' ngon ', u'hàg': u' hàng ', u'qủa': u' quả ','iu': u' yêu ','pv':u' phục vụ','nv':u' nhân viên','nc':u' nước',\n",
    "        u' hó ':u' khó',u' hắc ':u' khắc ','_': ' ', u' ng ':u' người '}\n",
    "\n",
    "    for k, v in replace_list.items():\n",
    "        document = document.replace(k, v)\n",
    "    # Loại bỏ Hashtag\n",
    "    document = re.sub(r'#([^\\s]+)', '', document)\n",
    "    # Loại bỏ các URL\n",
    "    document = re.sub(r'http([^\\s]+)', '', document)\n",
    "    # Loại bỏ number có trong comment\n",
    "    document = re.sub(r'\\d+', '', document)\n",
    "    #Remove duplicated characters: vd: đẹppppppp\n",
    "    document = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), document, flags=re.IGNORECASE)\n",
    "    # Loại bỏ các dấu câu, các ký tự đặc biệt (Emoji, icon cũng bị remove)\n",
    "    document = re.sub(r'[^\\w\\s]','', document)\n",
    "    # Chuyển chữ in hoa thành chữ thường\n",
    "    document = document.lower()\n",
    "    # Loại bỏ các ký tự dư thừa đơn lẻ (Nhiều ký tự đơn lẻ liền nhau kg xử lý được)\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Loại bỏ các Emoji, icon\n",
    "    document = re.sub(r'[^\\w\\s,]', '', document)\n",
    "    # Loại bỏ nhiều khoảng trắng dư thừa\n",
    "     #document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Tokenization\n",
    "    document = ViTokenizer.tokenize(document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đặt đường dẫn để mở tất cả các file .txt trong thư mục coment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyvi import ViTokenizer\n",
    "import glob\n",
    "\n",
    "#==========================================================\n",
    "# Tạo ra file NEG từ tất cả các file trong thư mục neg\n",
    "neg_paths = glob.glob(\"./data2/neg/*.txt\")\n",
    "neg_comments = []\n",
    "\n",
    "for path in neg_paths :\n",
    "  with open(path,encoding=\"utf-8\") as file:\n",
    "      text= file.read()\n",
    "      text = text_preprocessing(text)\n",
    "      neg_comments.append(text)\n",
    "\n",
    "\n",
    "# Chia dữ liệu thành 70:30 và lưu thành 2 file neg_train và ner_test\n",
    "neg_train, neg_test = train_test_split(neg_comments, test_size = 0.3, random_state = 42)\n",
    "\n",
    "with open(\"./doc2vec_data2/neg_train.txt\", 'w', encoding=\"utf-8\") as output:\n",
    "    for row in neg_train:\n",
    "        output.write(str(row) + '\\n')\n",
    "        \n",
    "with open(\"./doc2vec_data2/neg_test.txt\", 'w', encoding=\"utf-8\") as output:\n",
    "    for row in neg_test:\n",
    "        output.write(str(row) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========================================================\n",
    "\n",
    "# Tạo ra file POS từ tất cả các file trong thư mục pos\n",
    "pos_paths = glob.glob(\"./data2/pos/*.txt\")\n",
    "pos_comments = []\n",
    "\n",
    "for path in pos_paths :\n",
    "  with open(path,encoding=\"utf-8\") as file:\n",
    "      text= file.read()\n",
    "      text = text_preprocessing(text)\n",
    "      pos_comments.append(text)\n",
    "        \n",
    "# Chia dữ liệu thành 70:30 và lưu thành 2 file pos_train và pos_test\n",
    "pos_train, pos_test = train_test_split(pos_comments, test_size = 0.3, random_state = 42)\n",
    "\n",
    "with open(\"./doc2vec_data2/pos_train.txt\", 'w', encoding=\"utf-8\") as output:\n",
    "    for row in pos_train:\n",
    "        output.write(str(row) + '\\n')\n",
    "        \n",
    "with open(\"./doc2vec_data2/pos_test.txt\", 'w', encoding=\"utf-8\") as output:\n",
    "    for row in pos_test:\n",
    "        output.write(str(row) + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xây dựng mô hình Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, sources):\n",
    "        self.sources = sources\n",
    "        \n",
    "        flipped = {}\n",
    "        \n",
    "        # make sure that keys are unique\n",
    "        for key, value in sources.items():\n",
    "            if value not in flipped:\n",
    "                flipped[value] = [key]\n",
    "            else:\n",
    "                raise Exception('Non-unique prefix encountered')\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for source, prefix in self.sources.items():\n",
    "            with open(source, encoding=\"utf-8\") as fin: \n",
    "                for item_no, line in enumerate(fin):\n",
    "                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n",
    "    \n",
    "    def to_array(self):\n",
    "        self.sentences = []\n",
    "        for source, prefix in self.sources.items():\n",
    "            with open(source, encoding=\"utf-8\") as fin:\n",
    "                for item_no, line in enumerate(fin):\n",
    "                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n",
    "        return self.sentences\n",
    "    \n",
    "    def sentences_perm(self):\n",
    "        shuffled = list(self.sentences)\n",
    "        random.shuffle(shuffled)\n",
    "        return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok bây giờ chúng ta feed data files vào `LabeledLineSentence`,\n",
    "# `LabeledLineSentence` input 1 dict với key là tên file, value là `prefix` của các sentences trong văn bản.  \n",
    "sources = {\n",
    "    'doc2vec_data2/neg_test.txt':'TEST_NEG',\n",
    "    'doc2vec_data2/pos_test.txt':'TEST_POS', \n",
    "    'doc2vec_data2/neg_train.txt':'TRAIN_NEG', \n",
    "    'doc2vec_data2/pos_train.txt':'TRAIN_POS'\n",
    "}\n",
    "\n",
    "sentences = LabeledLineSentence(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "C:\\Users\\ADMIN\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n"
     ]
    }
   ],
   "source": [
    "# Building the Vocabulary Table\n",
    "\n",
    "model = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=7)\n",
    "model.build_vocab(sentences.to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspecting the Model\n",
    "\n",
    "#model.most_similar('phục_vụ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo training data và được word embdding bằng Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính tổng số training_data vì ở trên mình đã chia tỷ lệ 70:30\n",
    "len_X_train = len(pos_train) + len(neg_train)\n",
    "\n",
    "# Tạo training data và được word embdding bằng Doc2Vec\n",
    "X_train = np.zeros((len_X_train, 300))\n",
    "y_train = np.zeros(len_X_train)\n",
    "\n",
    "# Ở đây lấy giá trị neg_train hay pos_train cũng được, vì số lượng pos và neg bằng nhau\n",
    "for i in range(len(neg_train)):\n",
    "    prefix_train_pos = 'TRAIN_POS_' + str(i)\n",
    "    prefix_train_neg = 'TRAIN_NEG_' + str(i)\n",
    "    X_train[i] = model.docvecs[prefix_train_pos]\n",
    "    X_train[len(neg_train) + i] = model.docvecs[prefix_train_neg]\n",
    "    y_train[i] = 1\n",
    "    y_train[len(neg_train) + i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo testing data và được word embdding bằng Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tính tổng số testing_data vì ở trên mình đã chia tỷ lệ 70:30\n",
    "len_X_test = len(pos_test) + len(neg_test)\n",
    "\n",
    "# Tạo testing data và được word embdding bằng Doc2Vec\n",
    "X_test = np.zeros((len_X_test, 300))\n",
    "y_test = np.zeros(len_X_test)\n",
    "\n",
    "# Ở đây lấy giá trị neg_test hay pos_test cũng được, vì số lượng pos và neg bằng nhau\n",
    "for i in range(len(neg_test)):\n",
    "    prefix_test_pos = 'TEST_POS_' + str(i)\n",
    "    prefix_test_neg = 'TEST_NEG_' + str(i)\n",
    "    X_test[i] = model.docvecs[prefix_test_pos]\n",
    "    X_test[len(neg_test) + i] = model.docvecs[prefix_test_neg]\n",
    "    y_test[i] = 1\n",
    "    y_test[len(neg_test) + i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  0.009993791580200195  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng NaiveBayes\n",
    "\n",
    "import time\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_NB = time.time()\n",
    "\n",
    "\n",
    "classifier_NB = BernoulliNB(binarize=None)\n",
    "classifier_NB.fit(X_train, y_train)\n",
    "\n",
    "end_NB = time.time()\n",
    "time_NB = end_NB-start_NB\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_NB), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  192.6567759513855  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng Random Forest\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_RF = time.time()\n",
    "\n",
    "classifier_RF = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier_RF.fit(X_train, y_train) \n",
    "\n",
    "end_RF = time.time()\n",
    "time_RF = end_RF-start_RF\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_RF), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện:  28.953555822372437  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng SVM\n",
    "import time\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Thêm biến này vào để kiểm tra thời gian huấn luyện\n",
    "start_SVM = time.time()\n",
    "\n",
    "classifier_SVM = SVC(kernel = 'linear', random_state = 0)\n",
    "classifier_SVM.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "end_SVM = time.time()\n",
    "time_SVM = end_SVM-start_SVM\n",
    "\n",
    "#Show ra thời gian huấn luyện của mô hình\n",
    "#print(\"Thời gian huấn luyện: %s second\" % str(end-start))\n",
    "print(\"Thời gian huấn luyện: \", str(time_SVM), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện Logistic Regression:  1.6249902248382568  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "\n",
    "classifier_LR = LogisticRegression(fit_intercept=True, n_jobs=4)\n",
    "classifier_LR.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Thời gian huấn luyện Logistic Regression: \", str(end-start), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thời gian huấn luyện KNN:  2.6323866844177246  giây\n"
     ]
    }
   ],
   "source": [
    "# Xây dựng mô hình huấn luyện bằng KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "classifier_KNN = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier_KNN.fit(X_train, y_train)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"Thời gian huấn luyện KNN: \", str(end-start), \" giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Sentiment\n",
    "\n",
    "y_pred_NB = classifier_NB.predict(X_test)\n",
    "\n",
    "y_pred_RF = classifier_RF.predict(X_test)\n",
    "\n",
    "y_pred_SVM = classifier_SVM.predict(X_test)\n",
    "\n",
    "y_Pred_LR = classifier_LR.predict(X_test)\n",
    "\n",
    "y_Pred_KNN = classifier_KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy LogisticRegression:  0.4866666666666667\n",
      "Accuracy of Naive Bayes: 0.49233333333333335\n",
      "Accuracy KNN:  0.5186666666666667\n",
      "Accuracy of Random Forest: 0.489\n",
      "Accuracy of SVM: 0.48733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print('Accuracy LogisticRegression: ', accuracy_score(y_test, y_Pred_LR))\n",
    "print('Accuracy of Naive Bayes:',accuracy_score(y_test, y_pred_NB))\n",
    "print('Accuracy KNN: ', accuracy_score(y_test, y_Pred_KNN))\n",
    "print('Accuracy of Random Forest:',accuracy_score(y_test, y_pred_RF))\n",
    "print('Accuracy of SVM:',accuracy_score(y_test, y_pred_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
